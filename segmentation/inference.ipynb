{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apex is not installed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mmcv import Config\n",
    "from mmseg.datasets import build_dataloader, build_dataset\n",
    "from mmseg.models import build_segmentor\n",
    "from mmseg.apis import single_gpu_test\n",
    "from mmcv.runner import load_checkpoint\n",
    "from mmcv.parallel import MMDataParallel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to set\n",
    "CONFIG_PATH = '/opt/ml/input/level2_semanticsegmentation_cv-level2-cv-16/mmsegmentation/configs/_TrashSEG_/swin/upernet_swin_large_patch4_window7_512x512_pretrain_224x224_22K_160k_ade20k.py'\n",
    "TEST_IMAGES_PATH = '/opt/ml/input/data/test_images'\n",
    "ITER = 'best_mIoU_epoch_58'\n",
    "WORK_DIR = '/opt/ml/input/level2_semanticsegmentation_cv-level2-cv-16/mmsegmentation/work_dirs/upernet_swin_large_patch4_window7_512x512_pretrain_224x224_22K_160k_ade20k_0/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config.fromfile(CONFIG_PATH)\n",
    "root=TEST_IMAGES_PATH\n",
    "\n",
    "# dataset config 수정\n",
    "cfg.data.test.img_dir = root\n",
    "cfg.data.test.pipeline[1]['img_scale'] = (512,512) # Resize\n",
    "cfg.data.test.test_mode = True\n",
    "cfg.data.samples_per_gpu = 1\n",
    "cfg.work_dir = WORK_DIR\n",
    "# cfg.optimizer_config.grad_clip = dict(max_norm=35, norm_type=2)\n",
    "cfg.model.train_cfg = None\n",
    "\n",
    "# checkpoint path\n",
    "checkpoint_path = os.path.join(cfg.work_dir, f'{ITER}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 09:30:01,352 - mmseg - INFO - Loaded 819 images\n",
      "/opt/ml/input/level2_semanticsegmentation_cv-level2-cv-16/mmsegmentation/mmseg/models/losses/cross_entropy_loss.py:235: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: /opt/ml/input/level2_semanticsegmentation_cv-level2-cv-16/mmsegmentation/work_dirs/upernet_swin_large_patch4_window7_512x512_pretrain_224x224_22K_160k_ade20k_0/best_mIoU_epoch_58.pth\n"
     ]
    }
   ],
   "source": [
    "dataset = build_dataset(cfg.data.test)\n",
    "if len(dataset) != 819:\n",
    "        raise AssertionError('Test dataset should 819 image. Check your test.json')\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        samples_per_gpu=1,\n",
    "        workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "\n",
    "model = build_segmentor(cfg.model, test_cfg=cfg.get('test_cfg'))\n",
    "checkpoint = load_checkpoint(model, checkpoint_path, map_location='cpu')\n",
    "model.CLASSES = dataset.CLASSES\n",
    "model = MMDataParallel(model.cuda(), device_ids=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 819/819, 6.1 task/s, elapsed: 134s, ETA:     0s"
     ]
    }
   ],
   "source": [
    "output = single_gpu_test(model, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# sample_submisson.csv 열기\n",
    "submission = pd.read_csv('sample_submission.csv', index_col=None)\n",
    "json_dir = os.path.join(\"/opt/ml/input/data/test.json\")\n",
    "with open(json_dir, \"r\", encoding=\"utf8\") as outfile:\n",
    "    datas = json.load(outfile)\n",
    "\n",
    "# set resize\n",
    "input_size = 512\n",
    "output_size = 256\n",
    "transformed = A.Compose([A.Resize(output_size, output_size)])\n",
    "\t\t\n",
    "\n",
    "# PredictionString 대입\n",
    "for image_id, predict in enumerate(output):\n",
    "    image_id = datas[\"images\"][image_id]\n",
    "    file_name = image_id[\"file_name\"]\n",
    "    temp_mask = []\n",
    "    mask = np.array(predict, dtype='uint8')\n",
    "    mask = transformed(image=mask)\n",
    "    temp_mask.append(mask['image'])\n",
    "    oms = np.array(temp_mask)\n",
    "    oms = oms.reshape([oms.shape[0], output_size*output_size]).astype(int)\n",
    "\n",
    "    string = oms.flatten()\n",
    "    submission = submission.append({\"image_id\" : file_name, \"PredictionString\" : ' '.join(str(e) for e in string.tolist())}, \n",
    "                                   ignore_index=True)\n",
    "\n",
    "# submission.csv로 저장\n",
    "submission.to_csv(os.path.join(cfg.work_dir, f'submission_{ITER}.csv'), index=False)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d8d8e2b",
   "metadata": {},
   "source": [
    "## Show image results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70726f0b-7286-4eb6-8a44-d21f62f79dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.apis.inference import init_segmentor\n",
    "from mmdet.apis.inference import inference_segmentor\n",
    "from mmdet.apis.inference import show_result_pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0160e9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config.fromfile('./configs/_trashDet_/dyhead/atss_swin-l-p4-w12_fpn_dyhead_mstrain_2x_coco_aug_3.py')\n",
    "checkpoint_path = os.path.join(cfg.work_dir, f'{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd54cca8-33c3-4751-b15d-3869b45daf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_segmentor(cfg, checkpoint=checkpoint_path, device='cuda:0')\n",
    "# model.CLASSES = dataset.CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92215b6-0a50-4b6b-9053-15b9d8e75ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "imgs_idx = [0, 1, 2, 5, 11, 12] # , 11, 12 ,13 ,14, 15\n",
    "\n",
    "for idx in imgs_idx:\n",
    "    imgs.append(f'../../dataset/test/{idx:04d}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d20abd-b74f-4158-96bf-d6d81bdefbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = inference_segmentor(model, imgs)\n",
    "\n",
    "for i in range(len(imgs)):\n",
    "    show_result_pyplot(model, imgs[i], results[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hard voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path as osp\n",
    "import pandas as pd\n",
    "#import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = ''\n",
    "target1 = 'submission_best_iter_80000.csv'\n",
    "target2 = 'submission_best_mIoU_iter_52000.csv'\n",
    "target3 = 'submission_iter_80000.csv'\n",
    "\n",
    "df1 = pd.read_csv(osp.join(base_dir, target3))\n",
    "df2 = pd.read_csv(osp.join(base_dir, target1))\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['image_id'] = df1['image_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [ \n",
    "    'Background', 'General trash', 'Paper', 'Paper pack', 'Metal', 'Glass',\n",
    "    'Plastic', 'Styrofoam', 'Plastic bag', 'Battery', 'Clothing'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 819/819 [00:31<00:00, 26.04it/s]\n"
     ]
    }
   ],
   "source": [
    "PredictionString = []\n",
    "\n",
    "for idx in tqdm(range(len(df1))):\n",
    "    first = df1['PredictionString'][idx].split(' ')\n",
    "    second = df2['PredictionString'][idx].split(' ')\n",
    "\n",
    "    for i in range(len(first)):\n",
    "        if CLASSES[int(second[i])] == 'Glass' or CLASSES[int(second[i])] == 'Paper pack':\n",
    "            first[i] = second[i]\n",
    "            \n",
    "    result = ' '.join(first)\n",
    "    PredictionString.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['PredictionString'] = PredictionString\n",
    "submission.to_csv('./hard_voted_output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  4 2020, 07:30:14) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
